From these diagnostic plots we can evaluate how well each simulation reproduces the real dataset and in which ways it differs. One way to make comparisons is to look at the overall distributions (left column). Alternatively we can choose a reference (in this case the real data) and look at departures from that data, either as distributions (middle column) or quantile-quantile plots (right columns). Looking at the mean expression levels across genes we see that Lun and scDD simulations are missing lowly expressed genes while the Simple and Lun 2 simulations are skewed in this direction. The Splat simulation is a better match, likely due to the addition of high-expression outlier genes. Both versions of the Lun 2 simulation produce extremely highly variable genes, an effect which is seen to a lesser extent in the Lun simulation. The difference in variance is reflected in the mean-variance relationship where genes from the Lun 2 simulation are much too variable at high expression levels. Library size is another aspect in which the simulations differ from the real data. The simulations which don't contain a library size component (Simple, Lun, scDD) have different median library sizes and much smaller spreads. In this example the Lun 2 simulations also produce much larger library sizes.

One of the biggest differences between scRNA-seq and bulk RNA-seq experiments is the high number of observed zeros, where no expression is recorded for a particular gene in a particular cell. To properly recreate an scRNA-seq dataset a simulation must produce the correct number of zeros but also have them properly distributed across both genes and cells. In addition a relationship between the expression level of a gene and the number of observed zeros has been established and this should also be reproduced in simulations [ZEROS FIGURE REF]. The Simple and Lun 2 simulations produce too many zeros across both genes and cells while the Lun and scDD simulations produce too few. Interestingly not including dropout in the Splat simulation produces a better fit, suggesting that additional dropout is not present in the Tung dataset. The simulations generally perform well at emulating the relationship between expression level and percentage of zeros per gene, expect for Lun 2 simulations which have far too many zeros at high expression levels, likely related to the excess variance observed previously.

While this view allows us to visually inspect how simulations compare on a single dataset it does not give a clear indication of which simulation performs best on each metric or allow us to compare simulations across datasets. In order to do this we calculated a ranked MAD for each metric by ordering both the simulated and real values then taking the difference between them. For example to get a MAD for the means the mean expression for both the real data and the simulations were sorted, the real values were then subtracted from the simulated values and the median of the absolute differences taken as the final statistic. We then ranked the MADs for each metric to compare the simulations. Figure \ref{fig:ranks} summarises the results as a heatmap.